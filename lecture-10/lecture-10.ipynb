{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative solutions of equations\n",
    "\n",
    "Consider the following simple problem: assume we have a real valued function $f(x)$ and we would like to solve the equation \n",
    "\n",
    "$$ f(x) = c $$\n",
    "\n",
    "for some constant $c$.  Let us also assume that we made a guess $f(x_0) = c$.  Of course, unless we are extremely lucky, we are not going to hit the result. So, there will be an error:\n",
    "\n",
    "$$ f(x_0) = c + \\delta $$\n",
    "\n",
    "Now, using this error, let us improve our guess:\n",
    "\n",
    "$$ f(x_0) - \\delta = c $$\n",
    "\n",
    "But we want $\\delta$ to effect $x_0$.  Assuming we have a *local inverse* we get\n",
    "\n",
    "$$ f^{-1}(f(x_0) - \\delta) = f^{-1}(c) = a $$\n",
    "\n",
    "where $a$ is the solution we need to find.  Now, let us write the first order Taylor approximation for the left hand side:\n",
    "\n",
    "$$ x_0 - (f^{-1})'(x_0) \\cdot \\delta \\approx a $$\n",
    "\n",
    "and we know that $(f^{-1})'(x_0) = \\frac{1}{f'(x_0)}$\n",
    "\n",
    "So, our next best guess is going to be\n",
    "\n",
    "$$ x_1 = x_0 - \\frac{\\delta}{f'(x_0)} $$\n",
    "\n",
    "If we convert this formula into an iterative approximation, we get\n",
    "\n",
    "$$ x_{n+1} = x_n - \\frac{\\delta_n}{f'(x_n)} $$\n",
    "\n",
    "where $\\delta_n = f(x_n) - c$\n",
    "\n",
    "This algorithm is called [Newton-Raphson algorithm](https://en.wikipedia.org/wiki/Newton%27s_method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Solve(f, c, x0, eta=1e-2, n=10000):\n",
    "    for i in range(n):\n",
    "        delta = f(x0) - c\n",
    "        der = (f(x0+eta/2) - f(x0-eta/2))/eta\n",
    "        x1 = x0-delta/(der+eta*np.random.rand())\n",
    "        if(abs(x0-x1)<eta):\n",
    "            break\n",
    "        else: \n",
    "            x0 = x1\n",
    "    return([i,x1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn(x):\n",
    "    y = x*x\n",
    "    return(1.0 + math.cos(y+0.2)+math.log(0.24+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 1.7734318154171977, 1.24]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Solve(fn,1.24,1.0,1e-10)\n",
    "[x[0],x[1],fn(x[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple variable case (steepest descent)\n",
    "\n",
    "We can develop a similar algorithm for functions with several variables. That algorithm is called [steepest descent algorithm](https://ocw.mit.edu/courses/mathematics/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/lecture-notes/MIT18_409F09_scribe21.pdf):\n",
    "\n",
    "Given a function $F(x_1,\\ldots,x_n)$ the direction in which $F$ changes the most is the gradient of $F$ which is defined as\n",
    "\n",
    "$$ \\nabla \\cdot F = \\left(\\frac{\\partial F}{\\partial x_1},\\ldots,\\frac{\\partial F}{\\partial x_n}\\right) $$\n",
    "\n",
    "So, if we start with an initial guess $a^{(0)}$ for $F(a_1^{(0)},\\ldots,a_n^{(0)}) = c$, the update rule is going to be\n",
    "\n",
    "$$ a^{(m+1)} = a^{(m)} - \\eta \\left(\\nabla\\cdot F\\right)(a_1^{(m)},\\ldots,a_n^{(m)}) $$\n",
    "\n",
    "where $\\eta$ is called *the learning rate*.\n",
    "\n",
    "![](images/steepest_descent.png)\n",
    "\n",
    "(Image is taken from [\"Learning-Based Auditory Encoding for Robust Speech Recognition\" by Yu-Hsiang Bosco Chiu, Bhiksha Raj, and Richard M Stern](https://www.researchgate.net/figure/An-example-of-steepest-descent-optimization-steps_fig2_220655581)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(f,x,eta=1e-4):\n",
    "    def delta(i,j): \n",
    "        if(i==j): return(1) \n",
    "        else: return(0)\n",
    "    def der(i,eta=1e-4):\n",
    "        vec = np.array([delta(i,j) for j in range(len(x))])\n",
    "        x1 = x + vec*eta/2\n",
    "        x0 = x - vec*eta/2\n",
    "        return((f(x1) - f(x0) + eta*np.random.rand())/eta)\n",
    "    return(np.array([der(i,eta) for i in range(len(x))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSolve(f,c,x0,eta=1e-4,n=1000):\n",
    "    for i in range(n):\n",
    "        delta = f(x0) - c\n",
    "        x1 = x0 - delta*eta*grad(f,x0,eta)\n",
    "        err = np.linalg.norm(x1-x0)\n",
    "        if(err < eta):\n",
    "            break\n",
    "        else:\n",
    "            x0 = x1\n",
    "    return([i,x1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, array([6.31281606e-05, 5.00168448e-06])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def g(x):\n",
    "    y = x[0]*x[0]+x[1]*x[1]\n",
    "    return(1.0+math.atan(y)+math.log(1.0+y))\n",
    "\n",
    "MSolve(g,3.0,[0.0,0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The perceptron\n",
    "\n",
    "Consider the following problem now:\n",
    "\n",
    "Assume we have a collection of data points $(x^{(i)},y^{(i)})$ that satisfy a relationship of the form\n",
    "\n",
    "$$ y^{(i)} - f(\\alpha\\cdot x^{(i)} + \\beta) \\sim N(0,\\sigma) $$\n",
    "\n",
    "where $f\\colon\\mathbb{R}\\to\\mathbb{R}$ is a real valued function of a single variable, $\\alpha$ and $x^{(i)}$ are vectors in an inner product space and $\\beta$ is a scalar.  Our task is to find the best fitting pair $(\\alpha,\\beta)$ such that \n",
    "\n",
    "$$ \\sum_i (y^{(i)} - f(\\alpha\\cdot x^{(i)} + \\beta))^2 $$\n",
    "\n",
    "is minimized.\n",
    "\n",
    "This is a generalization of the [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) algorithm we covered in [Lecture 7](../lecture-7/lecture-7.ipynb).  In the logistic regression case $f(x) = \\frac{1}{1+e^{-x}}$.\n",
    "\n",
    "So, we proceed by an iterative update:\n",
    "\n",
    "$$ \\alpha^{(n+1)} = \\alpha^{(n)} - \\frac{\\eta \\delta^{(n)}}{f'(\\alpha^{(n)}\\cdot x^{(n)}+\\beta^{(n)})} x^{(n)} $$\n",
    "\n",
    "where $\\delta^{(n)} = f(\\alpha^{(n)}\\cdot x^{(n)} + \\beta^{(n)}) - y^{(n)}$\n",
    "\n",
    "### An example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.1833</td>\n",
       "      <td>0.2105</td>\n",
       "      <td>0.3039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0956</td>\n",
       "      <td>0.1321</td>\n",
       "      <td>0.1408</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.1710</td>\n",
       "      <td>0.0731</td>\n",
       "      <td>0.1401</td>\n",
       "      <td>0.2083</td>\n",
       "      <td>0.3513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0248</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.0548</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.0319</td>\n",
       "      <td>0.1158</td>\n",
       "      <td>0.0922</td>\n",
       "      <td>0.1027</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.2838</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0475</td>\n",
       "      <td>0.0647</td>\n",
       "      <td>0.0591</td>\n",
       "      <td>0.0753</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0173</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0671</td>\n",
       "      <td>0.1056</td>\n",
       "      <td>0.0697</td>\n",
       "      <td>0.0962</td>\n",
       "      <td>0.0251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8   \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "5  0.0286  0.0453  0.0277  0.0174  0.0384  0.0990  0.1201  0.1833  0.2105   \n",
       "6  0.0317  0.0956  0.1321  0.1408  0.1674  0.1710  0.0731  0.1401  0.2083   \n",
       "7  0.0519  0.0548  0.0842  0.0319  0.1158  0.0922  0.1027  0.0613  0.1465   \n",
       "8  0.0223  0.0375  0.0484  0.0475  0.0647  0.0591  0.0753  0.0098  0.0684   \n",
       "9  0.0164  0.0173  0.0347  0.0070  0.0187  0.0671  0.1056  0.0697  0.0962   \n",
       "\n",
       "       9  ...      51      52      53      54      55      56      57      58  \\\n",
       "0  0.2111 ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084  0.0090   \n",
       "1  0.2872 ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049  0.0052   \n",
       "2  0.6194 ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164  0.0095   \n",
       "3  0.1264 ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044  0.0040   \n",
       "4  0.4459 ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048  0.0107   \n",
       "5  0.3039 ...  0.0045  0.0014  0.0038  0.0013  0.0089  0.0057  0.0027  0.0051   \n",
       "6  0.3513 ...  0.0201  0.0248  0.0131  0.0070  0.0138  0.0092  0.0143  0.0036   \n",
       "7  0.2838 ...  0.0081  0.0120  0.0045  0.0121  0.0097  0.0085  0.0047  0.0048   \n",
       "8  0.1487 ...  0.0145  0.0128  0.0145  0.0058  0.0049  0.0065  0.0093  0.0059   \n",
       "9  0.0251 ...  0.0090  0.0223  0.0179  0.0084  0.0068  0.0032  0.0035  0.0056   \n",
       "\n",
       "       59  60  \n",
       "0  0.0032   0  \n",
       "1  0.0044   0  \n",
       "2  0.0078   0  \n",
       "3  0.0117   0  \n",
       "4  0.0094   0  \n",
       "5  0.0062   0  \n",
       "6  0.0103   0  \n",
       "7  0.0053   0  \n",
       "8  0.0022   0  \n",
       "9  0.0040   0  \n",
       "\n",
       "[10 rows x 61 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar = pd.read_csv(\"data/sonar.csv\",sep=\"\\t\",header=None)\n",
    "xs = sonar.iloc[:,0:60]\n",
    "ys = sonar.iloc[:,60]\n",
    "\n",
    "sonar.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(f,x,eta):\n",
    "    return((f(x+eta/2)-f(x-eta/2))/eta)\n",
    "\n",
    "def sigmoid(x): return(1.0/(1.0+math.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(xs, ys, f, epochs, batches, eta, tol):\n",
    "    n = len(xs.iloc[0,:])\n",
    "    num = len(xs)\n",
    "    w = np.random.rand(n+1)\n",
    "    err = []\n",
    "    temp = 0.0\n",
    "    for i in range(epochs):\n",
    "        j = np.random.randint(num)\n",
    "        x = xs.iloc[j,:]\n",
    "        y = ys[j]\n",
    "        x0 = np.append([1],x)\n",
    "        x1 = np.dot(w,x0)\n",
    "        delta = f(x1) - y\n",
    "        if(i%batches == batches-1):\n",
    "            err.append(temp)\n",
    "            temp = 0.0\n",
    "        elif(abs(delta) > tol):\n",
    "            temp = temp + 1.0/batches\n",
    "        der = diff(f,x1,eta)+eta*np.random.rand()\n",
    "        w = w - (der*delta*eta)*x0\n",
    "    return(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2028ca86a0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "errors = train(xs,ys,sigmoid,100000,1000,5e-1,0.45)\n",
    "plt.plot(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "\n",
    "A [neural network](https://en.wikipedia.org/wiki/Artificial_neural_network) is a directed graph of perceptrons:\n",
    "\n",
    "![](images/neural-net.png)\n",
    "\n",
    "(Source: [Wikipedia](https://en.wikipedia.org/wiki/Artificial_neural_network))\n",
    "\n",
    "In this case, the weights form a sequence of matrices (a tensor) $(w^i_{j,k})$.  In the feedforward phase of the network,  where the input $(x^i_j)$ at some layer $i$ is processed by the network as\n",
    "\n",
    "$$ x^{i+1}_k = \\sum_j f^i_k(x^i_j w^i_{j,k}) $$\n",
    "\n",
    "where $f^i_k$ is the activation function at the neuron $k$ at level $i$.\n",
    "\n",
    "There is a very large number of different types of neural networks.  You can find a good taxonomy of such networks [here](http://www.asimovinstitute.org/neural-network-zoo/):\n",
    "\n",
    "However, it is neither practical nor recommended that you implement neural networks by hand. \n",
    "\n",
    "![](images/meme.jpg)\n",
    "\n",
    "Use one of the following libraries or frameworks:\n",
    "\n",
    "1. [Theano](http://deeplearning.net/software/theano/)\n",
    "3. [TensorFlow](https://www.tensorflow.org/)\n",
    "4. [Caffe](http://caffe.berkeleyvision.org/) and [Caffe2](https://caffe2.ai/)\n",
    "2. [Keras](https://keras.io/)\n",
    "2. [MATLAB for Deep Learning](https://www.mathworks.com/campaigns/products/trials/targeted/dpl.html)\n",
    "5. [MXNet](https://mxnet.apache.org/)\n",
    "6. [The Microsoft Cognitive Toolkit](https://www.microsoft.com/en-us/cognitive-toolkit/)\n",
    "8. [Deep Learning for Java](https://deeplearning4j.org/)\n",
    "\n",
    "## Theano\n",
    "\n",
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 400                                   \n",
    "feats = 784                               \n",
    "\n",
    "rng = np.random\n",
    "D = (rng.randn(N, feats), rng.randint(size=N, low=0, high=2))\n",
    "w = theano.shared(rng.randn(feats), name=\"w\")\n",
    "b = theano.shared(0., name=\"b\")\n",
    "\n",
    "x = T.dmatrix(\"x\")\n",
    "y = T.dvector(\"y\")\n",
    "\n",
    "p_1 = 1 / (1 + T.exp(-T.dot(x, w) - b))\n",
    "prediction = p_1 > 0.5      \n",
    "\n",
    "xent = -y * T.log(p_1) - (1-y) * T.log(1-p_1) \n",
    "cost = xent.mean() + 0.01 * (w ** 2).sum() \n",
    "\n",
    "gw, gb = T.grad(cost, [w, b])             \n",
    "                                          \n",
    "train = theano.function(\n",
    "          inputs=[x,y],\n",
    "          outputs=[prediction, xent],\n",
    "          updates=((w, w - 0.1 * gw), (b, b - 0.1 * gb)))\n",
    "\n",
    "training_steps = 10000\n",
    "for i in range(training_steps):\n",
    "    pred, err = train(D[0], D[1])\n",
    "\n",
    "predict = theano.function(inputs=[x], outputs=prediction)\n",
    "pred = predict(D[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       60\n",
      "True  NaN\n",
      "False NaN\n",
      "True  NaN\n",
      "False NaN\n",
      "True  NaN\n",
      "True  NaN\n",
      "True  NaN\n",
      "True  NaN\n",
      "False NaN\n",
      "True  NaN\n",
      "False NaN\n",
      "True  NaN\n",
      "True  NaN\n",
      "False NaN\n",
      "True  NaN\n",
      "True  NaN\n",
      "False NaN\n",
      "False NaN\n",
      "False NaN\n",
      "True  NaN\n",
      "The confusion matrix is:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [208, 400]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-1fe889ef5223>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The confusion matrix is:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \"\"\"\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 230\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [208, 400]"
     ]
    }
   ],
   "source": [
    "frame = pd.DataFrame(ys,pred)\n",
    "print(frame.head(20))\n",
    "\n",
    "print(\"The confusion matrix is:\")\n",
    "confusion_matrix(ys,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras \n",
    "\n",
    "A simple neural-net for binary classification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid', input_dim=60))\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='RMSprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "208/208 [==============================] - 0s 32us/step - loss: 0.5453 - acc: 0.7452\n",
      "Epoch 2/100\n",
      "208/208 [==============================] - 0s 29us/step - loss: 0.5454 - acc: 0.7404\n",
      "Epoch 3/100\n",
      "208/208 [==============================] - 0s 26us/step - loss: 0.5447 - acc: 0.7356\n",
      "Epoch 4/100\n",
      "208/208 [==============================] - 0s 28us/step - loss: 0.5439 - acc: 0.7500\n",
      "Epoch 5/100\n",
      "208/208 [==============================] - 0s 24us/step - loss: 0.5436 - acc: 0.7356\n",
      "Epoch 6/100\n",
      "208/208 [==============================] - 0s 26us/step - loss: 0.5435 - acc: 0.7356\n",
      "Epoch 7/100\n",
      "208/208 [==============================] - 0s 27us/step - loss: 0.5429 - acc: 0.7548\n",
      "Epoch 8/100\n",
      "208/208 [==============================] - 0s 48us/step - loss: 0.5423 - acc: 0.7548\n",
      "Epoch 9/100\n",
      "208/208 [==============================] - 0s 31us/step - loss: 0.5420 - acc: 0.7644\n",
      "Epoch 10/100\n",
      "208/208 [==============================] - 0s 27us/step - loss: 0.5415 - acc: 0.7548\n",
      "Epoch 11/100\n",
      "208/208 [==============================] - 0s 25us/step - loss: 0.5413 - acc: 0.7452\n",
      "Epoch 12/100\n",
      "208/208 [==============================] - 0s 33us/step - loss: 0.5406 - acc: 0.7548\n",
      "Epoch 13/100\n",
      "208/208 [==============================] - 0s 52us/step - loss: 0.5405 - acc: 0.7356\n",
      "Epoch 14/100\n",
      "208/208 [==============================] - 0s 34us/step - loss: 0.5397 - acc: 0.7500\n",
      "Epoch 15/100\n",
      "208/208 [==============================] - 0s 27us/step - loss: 0.5392 - acc: 0.7548\n",
      "Epoch 16/100\n",
      "208/208 [==============================] - 0s 33us/step - loss: 0.5388 - acc: 0.7500\n",
      "Epoch 17/100\n",
      "208/208 [==============================] - 0s 28us/step - loss: 0.5384 - acc: 0.7548\n",
      "Epoch 18/100\n",
      "208/208 [==============================] - 0s 28us/step - loss: 0.5380 - acc: 0.7596\n",
      "Epoch 19/100\n",
      "208/208 [==============================] - 0s 28us/step - loss: 0.5375 - acc: 0.7452\n",
      "Epoch 20/100\n",
      "208/208 [==============================] - 0s 29us/step - loss: 0.5373 - acc: 0.7548\n",
      "Epoch 21/100\n",
      "208/208 [==============================] - 0s 29us/step - loss: 0.5365 - acc: 0.7500\n",
      "Epoch 22/100\n",
      "208/208 [==============================] - 0s 48us/step - loss: 0.5362 - acc: 0.7500\n",
      "Epoch 23/100\n",
      "208/208 [==============================] - 0s 48us/step - loss: 0.5358 - acc: 0.7404\n",
      "Epoch 24/100\n",
      "208/208 [==============================] - 0s 28us/step - loss: 0.5360 - acc: 0.7596\n",
      "Epoch 25/100\n",
      "208/208 [==============================] - 0s 28us/step - loss: 0.5347 - acc: 0.7548\n",
      "Epoch 26/100\n",
      "208/208 [==============================] - 0s 29us/step - loss: 0.5343 - acc: 0.7596\n",
      "Epoch 27/100\n",
      "208/208 [==============================] - 0s 37us/step - loss: 0.5341 - acc: 0.7596\n",
      "Epoch 28/100\n",
      "208/208 [==============================] - 0s 38us/step - loss: 0.5342 - acc: 0.7404\n",
      "Epoch 29/100\n",
      "208/208 [==============================] - 0s 39us/step - loss: 0.5333 - acc: 0.7500\n",
      "Epoch 30/100\n",
      "208/208 [==============================] - 0s 39us/step - loss: 0.5327 - acc: 0.7500\n",
      "Epoch 31/100\n",
      "208/208 [==============================] - 0s 92us/step - loss: 0.5327 - acc: 0.7404\n",
      "Epoch 32/100\n",
      "208/208 [==============================] - 0s 36us/step - loss: 0.5319 - acc: 0.7500\n",
      "Epoch 33/100\n",
      "208/208 [==============================] - 0s 35us/step - loss: 0.5317 - acc: 0.7500\n",
      "Epoch 34/100\n",
      "208/208 [==============================] - 0s 73us/step - loss: 0.5313 - acc: 0.7500\n",
      "Epoch 35/100\n",
      "208/208 [==============================] - 0s 31us/step - loss: 0.5306 - acc: 0.7404\n",
      "Epoch 36/100\n",
      "208/208 [==============================] - 0s 30us/step - loss: 0.5307 - acc: 0.7452\n",
      "Epoch 37/100\n",
      "208/208 [==============================] - 0s 46us/step - loss: 0.5298 - acc: 0.7548\n",
      "Epoch 38/100\n",
      "208/208 [==============================] - 0s 43us/step - loss: 0.5295 - acc: 0.7452\n",
      "Epoch 39/100\n",
      "208/208 [==============================] - 0s 51us/step - loss: 0.5291 - acc: 0.7548\n",
      "Epoch 40/100\n",
      "208/208 [==============================] - 0s 29us/step - loss: 0.5289 - acc: 0.7548\n",
      "Epoch 41/100\n",
      "208/208 [==============================] - 0s 29us/step - loss: 0.5282 - acc: 0.7500\n",
      "Epoch 42/100\n",
      "208/208 [==============================] - 0s 34us/step - loss: 0.5279 - acc: 0.7452\n",
      "Epoch 43/100\n",
      "208/208 [==============================] - 0s 35us/step - loss: 0.5273 - acc: 0.7404\n",
      "Epoch 44/100\n",
      "208/208 [==============================] - 0s 37us/step - loss: 0.5270 - acc: 0.7452\n",
      "Epoch 45/100\n",
      "208/208 [==============================] - 0s 33us/step - loss: 0.5265 - acc: 0.7404\n",
      "Epoch 46/100\n",
      "208/208 [==============================] - 0s 47us/step - loss: 0.5261 - acc: 0.7452\n",
      "Epoch 47/100\n",
      "208/208 [==============================] - 0s 56us/step - loss: 0.5256 - acc: 0.7548\n",
      "Epoch 48/100\n",
      "208/208 [==============================] - 0s 32us/step - loss: 0.5253 - acc: 0.7500\n",
      "Epoch 49/100\n",
      "208/208 [==============================] - 0s 31us/step - loss: 0.5247 - acc: 0.7500\n",
      "Epoch 50/100\n",
      "208/208 [==============================] - 0s 62us/step - loss: 0.5244 - acc: 0.7548\n",
      "Epoch 51/100\n",
      "208/208 [==============================] - 0s 57us/step - loss: 0.5242 - acc: 0.7596\n",
      "Epoch 52/100\n",
      "208/208 [==============================] - 0s 42us/step - loss: 0.5235 - acc: 0.7404\n",
      "Epoch 53/100\n",
      "208/208 [==============================] - 0s 30us/step - loss: 0.5231 - acc: 0.7548\n",
      "Epoch 54/100\n",
      "208/208 [==============================] - 0s 75us/step - loss: 0.5227 - acc: 0.7500\n",
      "Epoch 55/100\n",
      "208/208 [==============================] - 0s 39us/step - loss: 0.5224 - acc: 0.7548\n",
      "Epoch 56/100\n",
      "208/208 [==============================] - 0s 33us/step - loss: 0.5222 - acc: 0.7356\n",
      "Epoch 57/100\n",
      "208/208 [==============================] - 0s 29us/step - loss: 0.5215 - acc: 0.7548\n",
      "Epoch 58/100\n",
      "208/208 [==============================] - 0s 26us/step - loss: 0.5212 - acc: 0.7548\n",
      "Epoch 59/100\n",
      "208/208 [==============================] - 0s 25us/step - loss: 0.5208 - acc: 0.7500\n",
      "Epoch 60/100\n",
      "208/208 [==============================] - 0s 30us/step - loss: 0.5206 - acc: 0.7548\n",
      "Epoch 61/100\n",
      "208/208 [==============================] - 0s 27us/step - loss: 0.5208 - acc: 0.7548\n",
      "Epoch 62/100\n",
      "208/208 [==============================] - 0s 69us/step - loss: 0.5197 - acc: 0.7452\n",
      "Epoch 63/100\n",
      "208/208 [==============================] - 0s 56us/step - loss: 0.5195 - acc: 0.7548\n",
      "Epoch 64/100\n",
      "208/208 [==============================] - 0s 44us/step - loss: 0.5192 - acc: 0.7548\n",
      "Epoch 65/100\n",
      "208/208 [==============================] - 0s 40us/step - loss: 0.5186 - acc: 0.7596\n",
      "Epoch 66/100\n",
      "208/208 [==============================] - 0s 36us/step - loss: 0.5182 - acc: 0.7500\n",
      "Epoch 67/100\n",
      "208/208 [==============================] - 0s 38us/step - loss: 0.5181 - acc: 0.7596\n",
      "Epoch 68/100\n",
      "208/208 [==============================] - 0s 46us/step - loss: 0.5178 - acc: 0.7500\n",
      "Epoch 69/100\n",
      "208/208 [==============================] - 0s 25us/step - loss: 0.5173 - acc: 0.7548\n",
      "Epoch 70/100\n",
      "208/208 [==============================] - 0s 32us/step - loss: 0.5168 - acc: 0.7548\n",
      "Epoch 71/100\n",
      "208/208 [==============================] - 0s 25us/step - loss: 0.5172 - acc: 0.7500\n",
      "Epoch 72/100\n",
      "208/208 [==============================] - 0s 42us/step - loss: 0.5160 - acc: 0.7548\n",
      "Epoch 73/100\n",
      "208/208 [==============================] - 0s 38us/step - loss: 0.5163 - acc: 0.7500\n",
      "Epoch 74/100\n",
      "208/208 [==============================] - 0s 35us/step - loss: 0.5155 - acc: 0.7596\n",
      "Epoch 75/100\n",
      "208/208 [==============================] - 0s 27us/step - loss: 0.5151 - acc: 0.7548\n",
      "Epoch 76/100\n",
      "208/208 [==============================] - 0s 32us/step - loss: 0.5149 - acc: 0.7548\n",
      "Epoch 77/100\n",
      "208/208 [==============================] - 0s 60us/step - loss: 0.5145 - acc: 0.7548\n",
      "Epoch 78/100\n",
      "208/208 [==============================] - 0s 52us/step - loss: 0.5141 - acc: 0.7596\n",
      "Epoch 79/100\n",
      "208/208 [==============================] - 0s 38us/step - loss: 0.5137 - acc: 0.7548\n",
      "Epoch 80/100\n",
      "208/208 [==============================] - 0s 37us/step - loss: 0.5133 - acc: 0.7596\n",
      "Epoch 81/100\n",
      "208/208 [==============================] - 0s 34us/step - loss: 0.5132 - acc: 0.7596\n",
      "Epoch 82/100\n",
      "208/208 [==============================] - 0s 35us/step - loss: 0.5124 - acc: 0.7548\n",
      "Epoch 83/100\n",
      "208/208 [==============================] - 0s 35us/step - loss: 0.5127 - acc: 0.7548\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208/208 [==============================] - 0s 29us/step - loss: 0.5119 - acc: 0.7548\n",
      "Epoch 85/100\n",
      "208/208 [==============================] - 0s 35us/step - loss: 0.5119 - acc: 0.7548\n",
      "Epoch 86/100\n",
      "208/208 [==============================] - 0s 32us/step - loss: 0.5113 - acc: 0.7596\n",
      "Epoch 87/100\n",
      "208/208 [==============================] - 0s 64us/step - loss: 0.5107 - acc: 0.7548\n",
      "Epoch 88/100\n",
      "208/208 [==============================] - 0s 40us/step - loss: 0.5104 - acc: 0.7548\n",
      "Epoch 89/100\n",
      "208/208 [==============================] - 0s 29us/step - loss: 0.5104 - acc: 0.7548\n",
      "Epoch 90/100\n",
      "208/208 [==============================] - 0s 30us/step - loss: 0.5096 - acc: 0.7548\n",
      "Epoch 91/100\n",
      "208/208 [==============================] - 0s 33us/step - loss: 0.5094 - acc: 0.7644\n",
      "Epoch 92/100\n",
      "208/208 [==============================] - 0s 40us/step - loss: 0.5091 - acc: 0.7548\n",
      "Epoch 93/100\n",
      "208/208 [==============================] - 0s 38us/step - loss: 0.5088 - acc: 0.7644\n",
      "Epoch 94/100\n",
      "208/208 [==============================] - 0s 31us/step - loss: 0.5091 - acc: 0.7596\n",
      "Epoch 95/100\n",
      "208/208 [==============================] - 0s 33us/step - loss: 0.5082 - acc: 0.7548\n",
      "Epoch 96/100\n",
      "208/208 [==============================] - 0s 56us/step - loss: 0.5081 - acc: 0.7596\n",
      "Epoch 97/100\n",
      "208/208 [==============================] - 0s 36us/step - loss: 0.5076 - acc: 0.7644\n",
      "Epoch 98/100\n",
      "208/208 [==============================] - 0s 36us/step - loss: 0.5072 - acc: 0.7548\n",
      "Epoch 99/100\n",
      "208/208 [==============================] - 0s 58us/step - loss: 0.5074 - acc: 0.7548\n",
      "Epoch 100/100\n",
      "208/208 [==============================] - 0s 26us/step - loss: 0.5068 - acc: 0.7596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f201c8d0fd0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xs, ys, epochs=100, batch_size=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append([1,2],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
